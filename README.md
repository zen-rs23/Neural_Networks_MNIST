# Neural Networks and Deep Learning Project

## Overview

This project is a comprehensive exploration of neural networks and deep learning techniques, conducted as part of the Deep Learning subject in the Master of Data Science and Innovation program at the University of Technology Sydney. The project consists of two main parts:

1. **Part A: Building a Perceptron from Scratch**
2. **Part B: Training Custom Neural Networks on the Japanese MNIST Dataset**

## Part A: Building a Perceptron from Scratch

In this section, we implemented a simple perceptron using Numpy. The perceptron was trained on a small dataset with three features. Key components included:
- Manual implementation of forward and backpropagation
- Use of only Numpy and Pandas libraries, without high-level machine learning frameworks

## Part B: Custom Neural Networks on the Japanese MNIST Dataset

For Part B, we focused on training custom neural networks to classify handwritten Hiragana characters from the Japanese MNIST dataset. This part involved:
- Preprocessing the dataset by reshaping, standardizing, and flattening images
- Designing various neural network architectures using PyTorch and TensorFlow
- Implementing dropout layers for regularization to prevent overfitting
- Experimenting with different hyperparameters, such as learning rates and dropout rates
- Achieving a final model accuracy of 93.42% on the test set

## Key Components

- **Data Preprocessing:** Reshaping images, standardization, and data type conversion for optimal neural network processing.
- **Neural Network Design:** Utilizing fully connected layers with ReLU activation functions, dropout layers for regularization, and Adam optimizer for efficient training.
- **Model Evaluation:** Comprehensive analysis of model performance, including accuracy metrics and confusion matrices.
- **Experiments:** Conducted multiple experiments to fine-tune the model and minimize overfitting, resulting in significant performance improvements.

## Results and Analysis

The project's results were evaluated based on accuracy and generalization capability. The final model demonstrated high accuracy and robust generalization, indicating effective training and appropriate use of regularization techniques.

## Conclusion

This project highlights the application of deep learning techniques in a practical setting, focusing on neural network design, training, and evaluation. The experience provided valuable insights into the challenges of overfitting, the importance of regularization, and the intricacies of hyperparameter tuning.

## Acknowledgments

Special thanks to the University of Technology Sydney for providing the resources and support necessary to complete this project.

## Contact

For any questions or inquiries, please contact [Rohit Sharma](https://www.linkedin.com/in/rohit-sharma-2459096002/).

